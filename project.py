# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18EsWVZaOwlZSgjBLn0KLrbSAnQ97IIid
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import cv2
import matplotlib.pyplot as plt
from scipy.stats import entropy
from skimage.metrics import structural_similarity as ssim
from multiprocessing import Pool
from sklearn.model_selection import ParameterGrid
from time import time
import pickle
import os
from skimage.metrics import structural_similarity as ssim
from sklearn.metrics import mean_absolute_error

# Function to compute KL divergence between two distributions
def compute_kl_divergence(p, q):
    """Compute KL divergence between two distributions."""
    return entropy(p, q)

# Function to optimize HR patch using KL divergence
def optimize_hr_patch(lr_patch, hr_patch, bins=256, max_iter=100, alpha=0.01):
    """Optimize an HR patch to match the LR patch distribution using KL divergence."""

    # Estimate LR distribution
    lr_dist, bin_edges = np.histogram(lr_patch, bins=bins, range=(0, 255), density=True)
    lr_dist += 1e-8  # Avoid zero probabilities

    # Initialize HR distribution
    hr_dist, _ = np.histogram(hr_patch, bins=bins, range=(0, 255), density=True)
    hr_dist += 1e-8  # Avoid zero probabilities

    for _ in range(max_iter):
        # Compute KL divergence
        grad = lr_dist - hr_dist

        # Map gradient to pixel intensity changes in the HR patch
        bin_indices = np.digitize(hr_patch.flatten(), bin_edges[:-1], right=True)
        updated_patch = hr_patch.flatten()

        for i, idx in enumerate(bin_indices):
            if idx < len(grad):  # Ensure bin index is valid
                updated_patch[i] += alpha * grad[idx]

        # Reshape the updated patch back to the original shape
        hr_patch = updated_patch.reshape(hr_patch.shape)

        # Recompute HR distribution after the update
        hr_dist, _ = np.histogram(hr_patch, bins=bins, range=(0, 255), density=True)
        hr_dist += 1e-8  # Avoid zero probabilities

    return hr_patch

# Function for Super-Resolution with KL divergence
def super_resolve_with_kl(lr_image, target_size, patch_size=(32, 32), upscale_factor=4, bins=256, alpha=0.01, max_iter=100):
    """Perform super-resolution using KL divergence to map distributions."""

    lr_h, lr_w, _ = lr_image.shape
    target_h, target_w = target_size

    # Calculate padding to make LR image evenly divisible by patch size
    pad_h = (patch_size[0] - (lr_h % patch_size[0])) % patch_size[0]
    pad_w = (patch_size[1] - (lr_w % patch_size[1])) % patch_size[1]

    # Pad LR image
    lr_image_padded = np.pad(lr_image, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant', constant_values=0)

    # Divide LR image into patches
    lr_patches = []
    for i in range(0, lr_image_padded.shape[0], patch_size[0]):
        for j in range(0, lr_image_padded.shape[1], patch_size[1]):
            patch = lr_image_padded[i:i+patch_size[0], j:j+patch_size[1], :]
            lr_patches.append(patch)

    lr_patches = np.array(lr_patches)

    # Initialize HR patches
    hr_patches = []
    hr_patch_size = (patch_size[0] * upscale_factor, patch_size[1] * upscale_factor)

    # Use multiprocessing for batch processing
    with Pool() as pool:
        hr_patches = pool.starmap(optimize_hr_patch, [(lr_patch, cv2.resize(lr_patch, hr_patch_size, interpolation=cv2.INTER_CUBIC), bins, max_iter, alpha) for lr_patch in lr_patches])

    # Reconstruct HR image from patches
    hr_patches = np.array(hr_patches)
    hr_rows = lr_image_padded.shape[0] // patch_size[0]
    hr_cols = lr_image_padded.shape[1] // patch_size[1]

    hr_image_padded = np.zeros((hr_rows * hr_patch_size[0], hr_cols * hr_patch_size[1], 3), dtype=np.float32)

    patch_idx = 0
    for i in range(hr_rows):
        for j in range(hr_cols):
            hr_image_padded[i*hr_patch_size[0]:(i+1)*hr_patch_size[0], j*hr_patch_size[1]:(j+1)*hr_patch_size[1], :] = hr_patches[patch_idx]
            patch_idx += 1

    # Remove padding from HR image
    hr_image = hr_image_padded[:target_h, :target_w, :]

    # Final resizing to exact target size
    hr_image = cv2.resize(hr_image, (target_w, target_h), interpolation=cv2.INTER_CUBIC)

    return np.clip(hr_image, 0, 255).astype(np.uint8)

# Function to display the image
def display_image(image, title="Image"):
    """Display image using matplotlib."""
    plt.imshow(image)
    plt.axis('off')
    plt.title(title)
    plt.show()

# Grid search for hyperparameter tuning
def grid_search(lr_image, target_size, upscale_factor=4):
    """Perform grid search to tune hyperparameters for KL divergence."""
    param_grid = {'patch_size' : [(32, 32), (16,16)],
        'alpha': [0.01, 0.05],
        'bins': [256, 512],
        'max_iter': [50, 40, 60]}

    best_score = -float('inf')  # We are maximizing SSIM, so use negative infinity
    best_params = None
    best_image = None

    # Resize LR image to match HR dimensions for SSIM calculation
    lr_resized = cv2.resize(lr_image, (target_size[1], target_size[0]), interpolation=cv2.INTER_CUBIC)

    start_time = time()

    for params in ParameterGrid(param_grid):
        print(f"Testing with params: {params}")
        hr_image = super_resolve_with_kl(lr_image, target_size, upscale_factor=upscale_factor, **params)

        # Compute SSIM
        try:
            score = ssim(lr_resized, hr_image, multichannel=True, win_size=5, channel_axis=-1)
        except ValueError as e:
            print(f"Error calculating SSIM with params {params}: {e}")
            continue

        if score > best_score:
            best_score = score
            best_params = params
            best_image = hr_image

    end_time = time()
    print(f"Grid search completed in {end_time - start_time:.2f} seconds")

    return best_image, best_params

class SuperResolutionModel:
    def __init__(self, upscale_factor=4):
        self.upscale_factor = upscale_factor

    def predict(self, lr_image):
        # Calculate target size (4 times the LR image dimensions)
        target_size = (lr_image.shape[0] * self.upscale_factor, lr_image.shape[1] * self.upscale_factor)
        # Perform grid search for hyperparameter tuning
        hr_image, best_params = grid_search(lr_image, target_size)
        return hr_image

    def save(self, filename):
        with open(filename, 'wb') as f:
            pickle.dump(self, f)

    @staticmethod
    def load(filename):
        with open(filename, 'rb') as f:
            return pickle.load(f)

# Initialize the model
model = SuperResolutionModel(upscale_factor=4)

# Save the model to a pickle file
model.save("super_resolution_model.pkl")
print("Model saved as super_resolution_model.pkl")

input_folder = "/content/drive/MyDrive/SuperResolution/LR"  # Change this to your folder containing LR images
output_folder = "/content/drive/MyDrive/SuperResolution/Reconstructed_HR"
gt_folder = "/content/drive/MyDrive/SuperResolution/HR"  # New folder to save HR images
os.makedirs(output_folder, exist_ok=True)

# Initialize lists to store SSIM and MAE scores
ssim_scores = []
mae_scores = []

# Loop through all images in the input folder and process them
for filename in os.listdir(input_folder):
    # Only process image files (e.g., .png, .jpg)
    if filename.endswith((".png", ".jpg", ".jpeg")):
        print(f"Processing {filename}")
        lr_image_path = os.path.join(input_folder, filename)

        # Read the LR image
        lr_image = cv2.imread(lr_image_path)
        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)

        # Predict the HR image using the model
        hr_image = model.predict(lr_image)

        # Define the ground truth HR image path
        gt_image_path = os.path.join(gt_folder, filename.replace('.png', '_gt.png').replace('.jpg', '_gt.jpg'))

        # Read the ground truth HR image
        gt_image = cv2.imread(gt_image_path)
        gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)

        # Ensure the ground truth image and predicted HR image have the same size
        if gt_image.shape != hr_image.shape:
            hr_image = cv2.resize(hr_image, (gt_image.shape[1], gt_image.shape[0]), interpolation=cv2.INTER_CUBIC)

        # Compute SSIM
        ssim_score = ssim(gt_image, hr_image, multichannel=True)
        ssim_scores.append(ssim_score)

        # Compute MAE
        mae_score = mean_absolute_error(gt_image.flatten(), hr_image.flatten())
        mae_scores.append(mae_score)

        # Save the reconstructed HR image
        output_image_path = os.path.join(output_folder, filename.replace('.png', '_hr.png').replace('.jpg', '_hr.jpg'))
        cv2.imwrite(output_image_path, cv2.cvtColor(hr_image, cv2.COLOR_RGB2BGR))

        print(f"Processed {filename}: SSIM = {ssim_score:.4f}, MAE = {mae_score:.4f}")

# Calculate average SSIM and MAE scores
avg_ssim = np.mean(ssim_scores)
avg_mae = np.mean(mae_scores)

print(f"\nAverage SSIM: {avg_ssim:.4f}")
print(f"Average MAE: {avg_mae:.4f}")